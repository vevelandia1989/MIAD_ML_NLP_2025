{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a653cba-f62b-45b8-be91-c0acce992164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "def train_rnn_model(texto):    \n",
    "    # --- Encode text to integer sequence and shift to start from 0 ---\n",
    "    [encoded] = np.array(tokenizer.texts_to_sequences([texto])) - 1\n",
    "    \n",
    "    # --- Prepare training dataset (use 90% of total characters) ---\n",
    "    train_size = dataset_size * 90 // 100\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "    \n",
    "    # --- Create overlapping windows of 101 tokens (100 input + 1 target) ---\n",
    "    n_steps = 100\n",
    "    window_length = n_steps + 1                  # Input length + 1-step-ahead target\n",
    "    dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    \n",
    "    # --- Shuffle and batch the dataset ---\n",
    "    np.random.seed(42)\n",
    "    tf.random.set_seed(42)\n",
    "    batch_size = 32\n",
    "    dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "    \n",
    "    # --- Split into (X, Y) where Y is X shifted one character ahead ---\n",
    "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "    \n",
    "    # --- One-hot encode the input; targets stay as integer indices ---\n",
    "    dataset = dataset.map(\n",
    "        lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "    # --- Prefetch for performance (asynchronous loading) ---\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "    # --- Print shape of one batch to verify format ---\n",
    "    for X_batch, Y_batch in dataset.take(1):\n",
    "        print(X_batch.shape, Y_batch.shape)\n",
    "\n",
    "    # --- Define a simple RNN model using two GRU layers ---\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                         dropout=0.2),  # Optional: recurrent_dropout can be added\n",
    "        keras.layers.GRU(128, return_sequences=True,\n",
    "                         dropout=0.2),  # Optional: recurrent_dropout can be added\n",
    "        keras.layers.TimeDistributed(\n",
    "            keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "    ])\n",
    "\n",
    "    # --- Compile and train the model ---\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "    history = model.fit(dataset, epochs=4)\n",
    "\n",
    "    return model\n",
    "\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    y_proba = model(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e139030c-dc86-40ad-9107-758daf6202cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8255754"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_educacion = pd.read_csv(\"noticias_educacion_sample.csv\")\n",
    "df_educacion['clase'] = 0\n",
    "df_politica = pd.read_csv(\"noticias_politica_sample.csv\")\n",
    "df_politica['clase'] = 1\n",
    "df_deportes = pd.read_csv(\"noticias_deportes_sample.csv\")\n",
    "df_deportes['clase'] = 2\n",
    "df_economia = pd.read_csv(\"noticias_economia_sample.csv\")\n",
    "df_economia['clase'] = 3\n",
    "df = pd.concat([df_educacion, df_politica, df_deportes, df_economia]).dropna().reset_index()\n",
    "\n",
    "todas_noticias = \" \".join(df['content'])\n",
    "len(todas_noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dde10ae-0823-4ce3-a1b4-40b52eaa775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenize the text at character level ---\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(todas_noticias)\n",
    "\n",
    "max_id = len(tokenizer.word_index)           # Number of distinct characters\n",
    "dataset_size = tokenizer.document_count      # Total number of characters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "164aa25b-0af3-4c01-9645-e397e86a516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8255754"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "606db575-a9d4-4533-8f69-02c1fa158b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5832d136-02d3-4cd4-872c-1629e2d5c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747223349.518683  364696 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 699 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 146) (32, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 11:49:11.846399: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747223356.963656  366135 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747223357.892085  366132 service.cc:148] XLA service 0x7f8e51c05e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747223357.892117  366132 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2025-05-14 11:49:17.898190: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1747223357.988756  366132 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 193925/Unknown - 2006s 10ms/step - loss: 1.4393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 66729/232190 [=======>......................] - ETA: 29:51 - loss: 1.3344"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213735/232190 [==========================>...] - ETA: 3:22 - loss: 1.3748"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164065/232190 [====================>.........] - ETA: 12:50 - loss: 1.3539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232190/232190 [==============================] - 2648s 11ms/step - loss: 1.3653\n"
     ]
    }
   ],
   "source": [
    "model = train_rnn_model(todas_noticias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "46244187-55fc-43a6-8e6b-0649a530f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el tiempo que serán enviser que 47.024 unidades que pasa un\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"el tiempo\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a60ef07-c9ea-49a1-b01c-84dd1f17e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mi nombre es gustavo petro, goupo régliza. la inversión. además, se de\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"mi nombre es gustavo\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9799e7d3-30be-42c6-af00-1aca6da731fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gustavo bolivar de la compañía de la compañía de la compañía de l\n"
     ]
    }
   ],
   "source": [
    "print(complete_text(\"gustavo bolivar\", temperature=0.1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
